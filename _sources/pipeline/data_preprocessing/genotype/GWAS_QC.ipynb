{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "functional-stone",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "# GWAS data QC workflow\n",
    "\n",
    "This workflow implements some prelimary data QC steps for PLINK input files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adolescent-privilege",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Overview\n",
    "\n",
    "This notebook includes workflow for\n",
    "\n",
    "- Computer kinship matrix in sample and estimate related individuals\n",
    "- Genotype and sample QC: by MAF, missing data and HWE\n",
    "- LD pruning\n",
    "\n",
    "## Input format\n",
    "\n",
    "1. Whole genome data in VCF format, or in PLINK bim/bed/fam bundle; Or,\n",
    "2. A list of per chromosome VCF data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "biblical-button",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Run this workflow\n",
    "\n",
    "Depending on the context of your problem, the workflow can be executed in two ways:\n",
    "\n",
    "1. Run `merge_plink` if necessary, to merge all samples first; then run `king` to perform kinship estimate and finally `qc` to do addition QC\n",
    "2. When you have a separate data-set for kinship estimate different from your genotype of interest, you can run `king`, followed by `qc`.\n",
    "\n",
    "In both cases, you should use the `*.related_remove` output from `king` as the `--remove_samples` parameter input for `qc` step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aging-ultimate",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Minimal working example\n",
    "\n",
    "FIXME: first specify which of the 2 scenarios this example is for, then show how to run it.\n",
    "\n",
    "### First scenario: estimate kinship\n",
    "\n",
    "```\n",
    "sos run ~/bioworkflows/GWAS/GWAS_QC.ipynb king\\\n",
    "    --cwd ~/output \\\n",
    "    --genoFile ~/MWE_AD/rename_chr22.bed \\\n",
    "    --name first \\ \n",
    "    --kinship 0.05\n",
    "```\n",
    "\n",
    "### First scenario: do qc\n",
    "\n",
    "```\n",
    "sos run ~/bioworkflows/GWAS/GWAS_QC.ipynb qc\\\n",
    "    --cwd ~/output \\\n",
    "    --genoFile ~/MWE_AD/rename_chr22.bed \\\n",
    "    --remove_samples ~/output/rename_chr22.first.related_remove \\\n",
    "    --maf_filter 0.5 \\\n",
    "    --geno_filter 0.2 \\\n",
    "    --mind_filter 0.1 \\\n",
    "    --hwe_filter 0.0 \\\n",
    "    --name first \\\n",
    "    --window 50 \\\n",
    "    --shift 10 \\\n",
    "    --r2 0.5 \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "steady-talent",
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: sos run GWAS_QC.ipynb [workflow_name | -t targets] [options] [workflow_options]\n",
      "  workflow_name:        Single or combined workflows defined in this script\n",
      "  targets:              One or more targets to generate\n",
      "  options:              Single-hyphen sos parameters (see \"sos run -h\" for details)\n",
      "  workflow_options:     Double-hyphen workflow-specific parameters\n",
      "\n",
      "Workflows:\n",
      "  king\n",
      "  qc\n",
      "  merge_plink\n",
      "\n",
      "Global Workflow Options:\n",
      "  --cwd VAL (as path, required)\n",
      "                        the output directory for generated files\n",
      "  --name VAL (as str, required)\n",
      "                        A string to identify your analysis run\n",
      "  --job-size 1 (as int)\n",
      "                        For cluster jobs, number commands to run per job\n",
      "  --walltime 5h\n",
      "                        Wall clock time expected\n",
      "  --mem 16G\n",
      "                        Memory expected\n",
      "  --numThreads 20 (as int)\n",
      "                        Number of threads\n",
      "  --container-lmm 'statisticalgenetics/lmm:1.8'\n",
      "                        Software container option\n",
      "\n",
      "Sections\n",
      "  king:                 Inference of relationships in the sample to remove\n",
      "                        closely related individuals\n",
      "    Workflow Options:\n",
      "      --kinship 0.0625 (as float)\n",
      "                        Filter based on kinship coefficient higher than a number\n",
      "                        (e.g first degree 0.25, second degree 0.125, third\n",
      "                        degree 0.0625)\n",
      "      --genoFile VAL (as path, required)\n",
      "                        Plink binary file\n",
      "  qc_1:                 Filter SNPs and select individuals\n",
      "    Workflow Options:\n",
      "      --remove-samples . (as path)\n",
      "                        The path to the file that contains the list of samples\n",
      "                        to remove (format FID, IID)\n",
      "      --keep-samples . (as path)\n",
      "                        The path to the file that contains the list of samples\n",
      "                        to keep (format FID, IID)\n",
      "      --keep-variants . (as path)\n",
      "                        The path to the file that contains the list of variants\n",
      "                        to keep\n",
      "      --maf-filter 0.01 (as float)\n",
      "                        MAF filter to use\n",
      "      --geno-filter 0.01 (as float)\n",
      "                        Maximum missingess per-variant\n",
      "      --mind-filter 0.02 (as float)\n",
      "                        Maximum missingness per-sample\n",
      "      --hwe-filter 5e-08 (as float)\n",
      "                        HWE filter\n",
      "      --genoFile  paths\n",
      "\n",
      "                        Plink binary files\n",
      "  qc_2:                 LD prunning and remove related individuals (both ind of\n",
      "                        a pair)\n",
      "    Workflow Options:\n",
      "      --window 50 (as int)\n",
      "                        Window size\n",
      "      --shift 10 (as int)\n",
      "                        Shift window every 10 snps\n",
      "      --r2 0.1 (as float)\n",
      "  qc_3:                 Merge all the .bed files into one bed file\n",
      "    Workflow Options:\n",
      "      --merged-prefix ''\n",
      "  merge_plink:\n",
      "    Workflow Options:\n",
      "      --merged-prefix VAL (as str, required)\n",
      "      --genoFile  paths\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sos run GWAS_QC.ipynb -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "buried-universal",
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [],
   "source": [
    "nohup sos run  /home/hs3163/GIT/xqtl-pipeline/pipeline/data_preprocessing/genotype/GWAS_QC.ipynb qc \\\n",
    "    --genotype_list /home/hs3163/GIT/ADSPFG-xQTL/MWE/mwe_genotype_list \\\n",
    "    --cwd ./ \\\n",
    "    --name \"Dry\" \\\n",
    "    --merged_prefix \"merge\" \\\n",
    "    --container_lmm \"/mnt/mfs/statgen/containers/lmm.sif\" &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affected-mercury",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[global]\n",
    "# the output directory for generated files\n",
    "parameter: cwd = path\n",
    "# A string to identify your analysis run\n",
    "parameter: name = f\"{cwd:b}\"\n",
    "# For cluster jobs, number commands to run per job\n",
    "parameter: job_size = 1\n",
    "# Wall clock time expected\n",
    "parameter: walltime = \"5h\"\n",
    "# Memory expected\n",
    "parameter: mem = \"16G\"\n",
    "# Number of threads\n",
    "parameter: numThreads = 20\n",
    "# Software container option\n",
    "parameter: container_lmm = 'statisticalgenetics/lmm:2.4'\n",
    "# use this function to edit memory string for PLINK input\n",
    "from sos.utils import expand_size\n",
    "cwd = f\"{cwd:a}\"\n",
    "import pandas as pd\n",
    "parameter: genotype_list = path\n",
    "geno_file_inv = pd.read_csv(genotype_list, sep = \"\\t\")\n",
    "genoFile = geno_file_inv[\"dir\"].values.tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "czech-sullivan",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "# Merge all the .bed files into one bed file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equipped-deadline",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[qc_1,merge_plink]\n",
    "parameter: merged_prefix = name\n",
    "input: genoFile , group_by = 'all'\n",
    "output: merged_bed = f\"{cwd}/{merged_prefix}.mergrd.bed\"\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads, tags = f'{step_name}_{_output:bn}'\n",
    "bash:  expand= \"${ }\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout', container=container_lmm\n",
    "    echo -e ${' '.join([str(x)[:-4] for x in _input[1:]])} | sed 's/ /\\n/g' > ${_output:n}.merge_list\n",
    "    plink \\\n",
    "    --bfile ${_input[0]:n} \\\n",
    "    --merge-list ${_output:n}.merge_list \\\n",
    "    --make-bed \\\n",
    "    --out ${_output:n} \\\n",
    "    --threads ${numThreads} \\\n",
    "    --memory ${int(expand_size(mem) * 0.9)/1e06}\n",
    "\n",
    "## Genotype and sample QC\n",
    "\n",
    "QC the genetic data based on MAF, sample and variant missigness and Hardy-Weinberg Equilibrium (HWE).\n",
    "\n",
    "In this step you may also provide a list of samples to keep, for example in the case when you would like to subset a sample based on their ancestries to perform independent analyses on each of these groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "variable-ocean",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# Merge all the .bed files into one bed file \n",
    "[qc_1,merge_plink]\n",
    "parameter: merged_prefix = name\n",
    "input: genoFile , group_by = 'all'\n",
    "output: merged_bed = f\"{cwd}/{merged_prefix}.mergrd.bed\"\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads, tags = f'{step_name}_{_output:bn}'\n",
    "bash:  expand= \"${ }\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout', container=container_lmm, volumes = [f\"{genoFile[0]}:{genoFile[0]}\"]\n",
    "    echo -e ${' '.join([str(x)[:-4] for x in _input[1:]])} | sed 's/ /\\n/g' > ${_output:n}.merge_list\n",
    "    plink \\\n",
    "    --bfile ${_input[0]:n} \\\n",
    "    --merge-list ${_output:n}.merge_list \\\n",
    "    --make-bed \\\n",
    "    --out ${_output:n} \\\n",
    "    --threads ${numThreads} \\\n",
    "    --memory ${int(expand_size(mem) * 0.9)/1e06}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bibliographic-recipient",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# Filter SNPs and select individuals \n",
    "[qc_2,basic_filter]\n",
    "# The path to the file that contains the list of samples to remove (format FID, IID)\n",
    "parameter: remove_samples = path('.')\n",
    "# The path to the file that contains the list of samples to keep (format FID, IID)\n",
    "parameter: keep_samples = path('.')\n",
    "# The path to the file that contains the list of variants to keep\n",
    "parameter: keep_variants = path('.')\n",
    "# minimum MAF filter to use. Notice that PLINK default is 0.01\n",
    "parameter: maf_filter = 0.01\n",
    "# maximum MAF filter to use\n",
    "parameter: maf_max_filter = 0\n",
    "# Maximum missingess per-variant\n",
    "parameter: geno_filter = 0.01\n",
    "# Maximum missingness per-sample\n",
    "parameter: mind_filter = 0.02\n",
    "# HWE filter \n",
    "parameter: hwe_filter = 5e-08\n",
    "\n",
    "fail_if(not (keep_samples.is_file() or keep_samples == path('.')), msg = f'Cannot find ``{keep_samples}``')\n",
    "fail_if(not (keep_variants.is_file() or keep_variants == path('.')), msg = f'Cannot find ``{keep_variants}``')\n",
    "fail_if(not (remove_samples.is_file() or remove_samples == path('.')), msg = f'Cannot find ``{remove_samples}``')\n",
    "\n",
    "output: f'{cwd}/{_input:bn}.filtered{\".extracted\" if keep_variants.is_file() else \"\"}.bed'\n",
    "task: trunk_workers = 1, walltime = walltime, mem = mem, cores = numThreads, tags = f'{step_name}_{_output:bn}'\n",
    "bash:  expand= \"${ }\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout', container=container_lmm \n",
    "    plink \\\n",
    "      --bfile ${_input:n} \\\n",
    "      ${('--maf %s' % maf_filter) if maf_filter > 0 else ''} \\\n",
    "      ${('--max-maf %s' % maf_max_filter) if maf_max_filter > 0 else ''} \\\n",
    "      ${('--geno %s' % geno_filter) if geno_filter > 0 else ''} \\\n",
    "      ${('--hwe %s' % hwe_filter) if hwe_filter > 0 else ''} \\\n",
    "      ${('--mind %s' % mind_filter) if mind_filter > 0 else ''} \\\n",
    "      ${('--keep %s' % keep_samples) if keep_samples.is_file() else \"\"} \\\n",
    "      ${('--remove %s' % remove_samples) if remove_samples.is_file() else \"\"} \\\n",
    "      ${('--extract %s' % keep_variants) if keep_variants.is_file() else \"\"} \\\n",
    "      --make-bed \\\n",
    "      --out ${_output:n} \\\n",
    "      --threads ${numThreads} \\\n",
    "      --memory ${int(expand_size(mem) * 0.9)/1e6}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "engaged-cliff",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# LD prunning and remove related individuals (both ind of a pair)\n",
    "[qc_3,LD_pruning]\n",
    "# Window size\n",
    "depends: sos_step(\"qc_1\")\n",
    "parameter: window = 50\n",
    "# Shift window every 10 snps\n",
    "parameter: shift = 10\n",
    "parameter: r2 = 0.1\n",
    "output: bed=f'{_input:n}.prune.bed', prune=f'{_input:n}.prune.in'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads, tags = f'{step_name}_{_output[0]:bn}'\n",
    "bash: container=container_lmm, expand= \"${ }\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout'\n",
    "    plink2 \\\n",
    "    --bfile ${_input:n} \\\n",
    "    --indep-pairwise ${window} ${shift} ${r2}  \\\n",
    "    --out ${_output[\"prune\"]:nn} \\\n",
    "    --threads ${numThreads} \\\n",
    "    --memory ${int(expand_size(mem) * 0.9)/1e6}\n",
    "   \n",
    "    plink2 \\\n",
    "    --bfile ${_input:n} \\\n",
    "    --extract ${_output['prune']} \\\n",
    "    --make-bed \\\n",
    "    --out ${_output['bed']:n} \\\n",
    "    --threads ${numThreads} \\\n",
    "    --memory ${int(expand_size(mem) * 0.9)/1e6}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forward-subscriber",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "involved-latvia",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "## Estimate kinship in the sample\n",
    "\n",
    "The output is a list of related individuals, as well as the kinship matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tutorial-royalty",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# Inference of relationships in the sample to remove closely related individuals\n",
    "[qc_4,king_1]\n",
    "# Plink binary file\n",
    "parameter: kinship = 0.0625\n",
    "output: f'{cwd}/{_input[0]:bn}.kin0'\n",
    "task: trunk_workers = 1, walltime = walltime, mem = mem, cores = numThreads, tags = f'{step_name}_{_output:bn}'\n",
    "bash: container=container_lmm, expand= \"${ }\", stderr = f'{_output}.stderr', stdout = f'{_output}.stdout'\n",
    "    plink2 \\\n",
    "      --bfile ${_input[0]:n} \\\n",
    "      --make-king-table \\\n",
    "      --king-table-filter ${kinship} \\\n",
    "      --out ${_output:n} \\\n",
    "      --threads ${numThreads} \\\n",
    "      --memory ${int(expand_size(mem) * 0.9)/1e6}\n",
    "      \n",
    "[qc_5,king_2]\n",
    "# Filter based on kinship coefficient higher than a number (e.g first degree 0.25, second degree 0.125, third degree 0.0625)\n",
    "parameter: kinship = 0.0625\n",
    "# If set to true, the unrelated individuals in a family will be kept without being reported. But this will take a significantly long time.\n",
    "# Otherwise (use `--no-maximize-unrelated`) the entire family will be removed\n",
    "parameter: maximize_unrelated = \"FALSE\"\n",
    "output: relate_id = f'{_input:n}.related_id'\n",
    "task: trunk_workers = 1, walltime = walltime, mem = mem, cores = numThreads, tags = f'{step_name}_{_output:bn}'\n",
    "R:  expand= \"${ }\", stderr = f'{_output}.stderr', stdout = f'{_output}.stdout',container=container_lmm\n",
    "    library(dplyr)\n",
    "    library(igraph)\n",
    "    # Remove related individuals while keeping maximum number of individuals\n",
    "    # this function is simplified from: \n",
    "    # https://rdrr.io/cran/plinkQC/src/R/utils.R\n",
    "    #' @param relatedness [data.frame] containing pair-wise relatedness estimates\n",
    "    #' (in column [relatednessRelatedness]) for individual 1 (in column\n",
    "    #' [relatednessID1] and individual 2 (in column [relatednessID1]). Columns\n",
    "    #' relatednessID1, relatednessID2 and relatednessRelatedness have to present,\n",
    "    #' while additional columns such as family IDs can be present. Default column\n",
    "    #' names correspond to column names in output of plink --genome\n",
    "    #' (\\url{https://www.cog-genomics.org/plink/1.9/ibd}). All original\n",
    "    #' columns for pair-wise highIBDTh fails will be returned in fail_IBD.\n",
    "    #' @param relatednessTh [double] Threshold for filtering related individuals.\n",
    "    #' Individuals, whose pair-wise relatedness estimates are greater than this\n",
    "    #' threshold are considered related.\n",
    "    relatednessFilter <- function(relatedness, \n",
    "                                  relatednessTh,\n",
    "                                  relatednessID1=\"ID1\", \n",
    "                                  relatednessID2=\"ID2\",\n",
    "                                  relatednessRelatedness=\"KINSHIP\") {\n",
    "        # format data\n",
    "        if (!(relatednessID1 %in% names(relatedness))) {\n",
    "            stop(paste(\"Column\", relatednessID1, \"for relatedness not found!\"))\n",
    "        }\n",
    "        if (!(relatednessID2 %in% names(relatedness))) {\n",
    "            stop(paste(\"Column\", relatednessID1, \"for relatedness not found!\"))\n",
    "        }\n",
    "        if (!(relatednessRelatedness %in% names(relatedness))) {\n",
    "            stop(paste(\"Column\", relatednessRelatedness,\n",
    "                       \"for relatedness not found!\"))\n",
    "        }\n",
    "\n",
    "        ID1_index <- which(colnames(relatedness) == relatednessID1)\n",
    "        ID2_index <- which(colnames(relatedness) == relatednessID2)\n",
    "\n",
    "        relatedness[,ID1_index] <- as.character(relatedness[,ID1_index])\n",
    "        relatedness[,ID2_index] <- as.character(relatedness[,ID2_index])\n",
    "\n",
    "        relatedness_names <- names(relatedness)\n",
    "        names(relatedness)[ID1_index] <- \"ID1\"\n",
    "        names(relatedness)[ID2_index] <- \"ID2\"\n",
    "        names(relatedness)[names(relatedness) == relatednessRelatedness] <- \"M\"\n",
    "\n",
    "        # Remove symmetric IID rows\n",
    "        relatedness_original <- relatedness\n",
    "        relatedness <- dplyr::select(relatedness, ID1, ID2, M)\n",
    "\n",
    "        sortedIDs <- data.frame(t(apply(relatedness, 1, function(pair) {\n",
    "            c(sort(c(pair[1], pair[2])))\n",
    "            })), stringsAsFactors=FALSE)\n",
    "        keepIndex <- which(!duplicated(sortedIDs))\n",
    "\n",
    "        relatedness_original <- relatedness_original[keepIndex,]\n",
    "        relatedness <- relatedness[keepIndex,]\n",
    "\n",
    "        # individuals with at least one pair-wise comparison > relatednessTh\n",
    "        # return NULL to failIDs if no one fails the relatedness check\n",
    "        highRelated <- dplyr::filter(relatedness, M > relatednessTh)\n",
    "        if (nrow(highRelated) == 0) {\n",
    "            return(list(relatednessFails=NULL, failIDs=NULL))\n",
    "        }\n",
    "\n",
    "        # all samples with related individuals\n",
    "        allRelated <- c(highRelated$ID1, highRelated$ID2)\n",
    "        uniqueIIDs <- unique(allRelated)\n",
    "\n",
    "        # Further selection of samples with relatives in cohort\n",
    "        multipleRelative <- unique(allRelated[duplicated(allRelated)])\n",
    "        singleRelative <- uniqueIIDs[!uniqueIIDs %in% multipleRelative]\n",
    "\n",
    "        highRelatedMultiple <- highRelated[highRelated$ID1 %in% multipleRelative |\n",
    "                                            highRelated$ID2 %in% multipleRelative,]\n",
    "        highRelatedSingle <- highRelated[highRelated$ID1 %in% singleRelative &\n",
    "                                           highRelated$ID2 %in% singleRelative,]\n",
    "\n",
    "        # Only one related samples per individual\n",
    "        if(length(singleRelative) != 0) {\n",
    "          # randomly choose one to exclude\n",
    "          failIDs_single <- highRelatedSingle[,1]\n",
    "            \n",
    "        } else {\n",
    "          failIDs_single <- NULL\n",
    "        }\n",
    "  \n",
    "        # An individual has multiple relatives\n",
    "        if(length(multipleRelative) != 0) {\n",
    "            relatedPerID <- lapply(multipleRelative, function(x) {\n",
    "                tmp <- highRelatedMultiple[rowSums(\n",
    "                    cbind(highRelatedMultiple$ID1 %in% x,\n",
    "                          highRelatedMultiple$ID2 %in% x)) != 0,1:2]\n",
    "                rel <- unique(unlist(tmp))\n",
    "                return(rel)\n",
    "            })\n",
    "            names(relatedPerID) <- multipleRelative\n",
    "\n",
    "            keepIDs_multiple <- lapply(relatedPerID, function(x) {\n",
    "                pairwise <- t(combn(x, 2))\n",
    "                index <- (highRelatedMultiple$ID1 %in% pairwise[,1] &\n",
    "                              highRelatedMultiple$ID2 %in% pairwise[,2]) |\n",
    "                    (highRelatedMultiple$ID1 %in% pairwise[,2] &\n",
    "                         highRelatedMultiple$ID2 %in% pairwise[,1])\n",
    "                combination <- highRelatedMultiple[index,]\n",
    "                combination_graph <- igraph::graph_from_data_frame(combination,\n",
    "                                                                   directed=FALSE)\n",
    "                all_iv_set <- igraph::ivs(combination_graph)\n",
    "                length_iv_set <- sapply(all_iv_set, function(x) length(x))\n",
    "\n",
    "                if (all(length_iv_set == 1)) {\n",
    "                    # check how often they occurr elsewhere\n",
    "                    occurrence <- sapply(x, function(id) {\n",
    "                        sum(sapply(relatedPerID, function(idlist) id %in% idlist))\n",
    "                    })\n",
    "                    # if occurrence the same everywhere, pick the first, else keep\n",
    "                    # the one with minimum occurrence elsewhere\n",
    "                    if (length(unique(occurrence)) == 1) {\n",
    "                        nonRelated <- sort(x)[1]\n",
    "                    } else {\n",
    "                        nonRelated <- names(occurrence)[which.min(occurrence)]\n",
    "                    }\n",
    "                } else {\n",
    "                    nonRelated <- all_iv_set[which.max(length_iv_set)]\n",
    "                }\n",
    "                return(nonRelated)\n",
    "            })\n",
    "            keepIDs_multiple <- unique(unlist(keepIDs_multiple))\n",
    "            failIDs_multiple <- c(multipleRelative[!multipleRelative %in%\n",
    "                                                       keepIDs_multiple])\n",
    "        } else {\n",
    "            failIDs_multiple <- NULL\n",
    "        }\n",
    "        allFailIIDs <- c(failIDs_single, failIDs_multiple)\n",
    "        relatednessFails <- lapply(allFailIIDs, function(id) {\n",
    "            fail_inorder <- relatedness_original$ID1 == id &\n",
    "                relatedness_original$M > relatednessTh\n",
    "            fail_inreverse <- relatedness_original$ID2 == id &\n",
    "                relatedness_original$M > relatednessTh\n",
    "            if (any(fail_inreverse)) {\n",
    "                inreverse <- relatedness_original[fail_inreverse, ]\n",
    "                id1 <- ID1_index\n",
    "                id2 <- ID2_index\n",
    "                inreverse[,c(id1, id2)] <- inreverse[,c(id2, id1)]\n",
    "                names(inreverse) <- relatedness_names\n",
    "            } else {\n",
    "                inreverse <- NULL\n",
    "            }\n",
    "            inorder <- relatedness_original[fail_inorder, ]\n",
    "            names(inorder) <- relatedness_names\n",
    "            return(rbind(inorder, inreverse))\n",
    "        })\n",
    "        relatednessFails <- do.call(rbind, relatednessFails)\n",
    "        if (nrow(relatednessFails) == 0) {\n",
    "            relatednessFails <- NULL\n",
    "            failIDs <- NULL\n",
    "        } else {\n",
    "            names(relatednessFails) <- relatedness_names\n",
    "            rownames(relatednessFails) <- 1:nrow(relatednessFails)\n",
    "            uniqueFails <- relatednessFails[!duplicated(relatednessFails[,ID1_index]),]\n",
    "            failIDs <- uniqueFails[,ID1_index]\n",
    "        }\n",
    "        return(list(relatednessFails=relatednessFails, failIDs=failIDs))\n",
    "    }\n",
    "    \n",
    "  \n",
    "    # main code\n",
    "    kin0 <- read.table(${_input:r}, header=F, stringsAsFactor=F)\n",
    "    colnames(kin0) <- c(\"FID1\",\"ID1\",\"FID2\",\"ID2\",\"NSNP\",\"HETHET\",\"IBS0\",\"KINSHIP\")\n",
    "    if (${maximize_unrelated}) {\n",
    "        rel <- relatednessFilter(kin0, ${kinship}, \"ID1\", \"ID2\", \"KINSHIP\")$failIDs\n",
    "        tmp1 <- kin0[,1:2]\n",
    "        tmp2 <- kin0[,3:4]\n",
    "        colnames(tmp1) = colnames(tmp2) = c(\"FID\", \"ID\")\n",
    "        # Get the family ID for these rels so there are two columns FID and IID in the output\n",
    "        lookup <- dplyr::distinct(rbind(tmp1,tmp2))\n",
    "        dat <- lookup[which(lookup[,2] %in% rel),]\n",
    "    } else {\n",
    "        rel <- kin0 %>% filter(KINSHIP >= ${kinship})\n",
    "        IID <- sort(unique(unlist(rel[, c(\"ID1\", \"ID2\")])))\n",
    "        dat <- data.frame(IID)\n",
    "        dat <- dat %>%\n",
    "            mutate(FID = IID) %>%\n",
    "            select(FID, IID)\n",
    "    }\n",
    "    cat(\"There are\", nrow(dat),\"related individuals using a kinship threshold of ${kinship}\\n\")\n",
    "    write.table(dat,${_output:r}, quote=FALSE, row.names=FALSE, col.names=FALSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shaped-kazakhstan",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[qc_6,sep_by_relateness]\n",
    "input: output_from(\"qc_5\"), output_from(\"qc_3\")[\"bed\"]\n",
    "output: unrelated_bed = f'{cwd}/{_input[1]:bn}.unrelated.bed',\n",
    "        related_bed = f'{cwd}/{_input[1]:bn}.related.bed'\n",
    "task: trunk_workers = 1, walltime = walltime, mem = mem, cores = numThreads, tags = f'{step_name}_{_output:bn}'\n",
    "bash:  expand= \"${ }\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout', container = container_lmm\n",
    "    plink2 \\\n",
    "      --bfile ${_input[1]:n} \\\n",
    "      --remove ${_input[0]} \\\n",
    "      --make-bed \\\n",
    "      --out ${_output[0]:n} \\\n",
    "      --threads ${numThreads} \\\n",
    "      --memory ${int(expand_size(mem) * 0.9)/1e6}\n",
    "\n",
    "    plink2 \\\n",
    "      --bfile ${_input[1]:n} \\\n",
    "      --keep ${_input[0]} \\\n",
    "      --make-bed \\\n",
    "      --out ${_output[1]:n} \\\n",
    "      --threads ${numThreads} \\\n",
    "      --memory ${int(expand_size(mem) * 0.9)/1e6}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transparent-introduction",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "nbdime-conflicts": {
   "local_diff": [
    {
     "diff": [
      {
       "diff": [
        {
         "key": 0,
         "op": "addrange",
         "valuelist": [
          "0.22.4"
         ]
        },
        {
         "key": 0,
         "length": 1,
         "op": "removerange"
        }
       ],
       "key": "version",
       "op": "patch"
      }
     ],
     "key": "sos",
     "op": "patch"
    }
   ],
   "remote_diff": [
    {
     "diff": [
      {
       "diff": [
        {
         "key": 0,
         "op": "addrange",
         "valuelist": [
          "0.22.6"
         ]
        },
        {
         "key": 0,
         "length": 1,
         "op": "removerange"
        }
       ],
       "key": "version",
       "op": "patch"
      }
     ],
     "key": "sos",
     "op": "patch"
    }
   ]
  },
  "sos": {
   "kernels": [
    [
     "Bash",
     "bash",
     "Bash",
     "#E6EEFF",
     ""
    ],
    [
     "SoS",
     "sos",
     "",
     "",
     "sos"
    ]
   ],
   "panel": {
    "displayed": true,
    "height": 0
   },
   "version": "0.22.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
