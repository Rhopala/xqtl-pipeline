{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "played-nicholas",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "# Summary statistics merger\n",
    "\n",
    "\n",
    "This notebook takes in more than one collections of sumstat RDS file,  to produce a collections of merged.rds files that can served as the input of both MASH and MVSuSiE analysis.\n",
    "\n",
    "Each of the input sumstat RDS file must be a list with bhat and sbhat table, and the rowname for each of the table must be snp name in the form of chr:pos_alt_ref\n",
    "\n",
    "Allele flip issues will also be detected and resolved in the process of merging.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hourly-rainbow",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[global]\n",
    "import glob\n",
    "# Path to work directory where the yaml file locates\n",
    "parameter: yml = path(\".\")\n",
    "# Containers that contains the necessary packages\n",
    "parameter: container = 'gaow/twas'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "micro-speed",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "### Merge multiple summary statistic files to new summary statistic files with common SNPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b8477102-e306-4df3-a78b-fdfef4d3839d",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "9c07c88c-79d7-45c8-9470-084bad0c68da",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "def merge_sumstats(yml):\n",
    "    #parse yaml\n",
    "    yml = load_yaml(yml)\n",
    "    input_dict = parse_input(yml['INPUT'])\n",
    "    target_dict = parse_input(yml['TARGET'])\n",
    "    output_path = yml['OUTPUT']\n",
    "    keep_ambiguous = yml['KEEP_AMBIGUOUS']\n",
    "    input_dict[list(target_dict.keys())[0]] = list(target_dict.values())\n",
    "    lst_sumstats_file = [os.path.basename(i) for i in input_dict.keys()]\n",
    "    print('Total number of sumstats: ',len(lst_sumstats_file))\n",
    "    if len(set(lst_sumstats_file))<len(lst_sumstats_file):\n",
    "        raise Exception(\"There are duplicated names in \", lst_sumstats_file)\n",
    "    #read all sumstats\n",
    "    lst_sumstats = {os.path.basename(i):read_sumstat(i,j) for i,j in input_dict.items()}\n",
    "    nqs = []\n",
    "    for query in lst_sumstats.values():\n",
    "        nq,_ = snps_match(query,lst_sumstats[list(target_dict.keys())[0]],keep_ambiguous)\n",
    "        nqs.append(nq)\n",
    "    #get common snps\n",
    "    common_snps = set.intersection(*[set(nq.SNP) for nq in nqs])\n",
    "    print('Total number of common SNPs: ',len(common_snps))\n",
    "    #write out new smustats\n",
    "    for output_sumstats,nq in zip(lst_sumstats_file,nqs):\n",
    "        sumstats = nq[nq.SNP.isin(common_snps)]\n",
    "        sumstats.to_csv(output_sumstats, sep = \"\\t\", header = True, index = False,compression='gzip')\n",
    "    print('All are done!!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "13b1e68b-11fc-4391-9e3a-b1fccfe52647",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "def load_yaml(yaml_file):\n",
    "    with open(yaml_file, \"r\") as stream:\n",
    "        try:\n",
    "            yml = yaml.safe_load(stream)\n",
    "        except yaml.YAMLError as exc:\n",
    "            print(exc)\n",
    "    return yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6f5b2610-a51d-4cca-82d1-647b34a96f8f",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "def parse_input(yml_input):\n",
    "    input_dict = {}\n",
    "    for i in yml_input:\n",
    "        for name in glob.glob(list(i.keys())[0]):\n",
    "            input_dict[name] = list(i.values())[0]\n",
    "    return input_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6ee1ac41-8ca5-42c8-865f-0ea977070240",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "def read_sumstat(self,file, config=None):\n",
    "    try:\n",
    "        sumstats = pd.read_csv(file, compression='gzip', header=0, sep='\\t', quotechar='\"')\n",
    "    except:\n",
    "        sumstats = pd.read_csv(file, header=0, sep='\\t', quotechar='\"')\n",
    "    if config is not None:\n",
    "        try:\n",
    "            sumstats = sumstats.loc[:,list(config.values())]\n",
    "        except:\n",
    "            raise ValueError(f'According to config_file, input summary statistics should have the following columns: %s' % list(config.values()))\n",
    "        sumstats.columns = list(config.keys())\n",
    "    sumstats.SNP = 'chr'+sumstats.CHR.astype(str) + ':' + sumstats.POS.astype(str) + ':' + sumstats.A0.astype(str) + ':' + sumstats.A1.astype(str)\n",
    "    sumstats.CHR = sumstats.CHR.astype(int)\n",
    "    sumstats.POS = sumstats.POS.astype(int)\n",
    "    return sumstats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1b8a91a8-a692-42b3-a65c-90a615a3f773",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "def snps_match(query,subject,keep_ambiguous=True):\n",
    "    query.index = query.iloc[:,:2].astype(str).agg(':'.join, axis=1)\n",
    "    subject.index = subject.iloc[:,:2].astype(str).agg(':'.join, axis=1)\n",
    "    #overlap snps by chr+pos\n",
    "    print(\"Total rows of query: \",query.shape[0],\"Total rows of subject: \",subject.shape[0])\n",
    "    subject = subject[subject.index.isin(query.index)]\n",
    "    query = query.loc[subject.index]\n",
    "    print(\"Overlap chr:pos\",query.shape[0])\n",
    "    if query.index.duplicated().any():\n",
    "        raise Exception(\"There are duplicated chr:pos\")\n",
    "    pm = pair_match(query.A1,query.A0,subject.A1,subject.A0)\n",
    "    if keep_ambiguous:\n",
    "        print('Warning: there are',sum(~pm.ambiguous),'ambiguous SNPs')\n",
    "        pm = pm.iloc[:,1:]\n",
    "    else:\n",
    "        pm = pm[~pm.ambiguous].iloc[:,1:]\n",
    "    keep_idx = pm.any(axis=1)\n",
    "    print(\"Overlap SNPs\",sum(keep_idx))\n",
    "    #overlap snps by chr+pos+alleles.\n",
    "    new_subject = subject[keep_idx]\n",
    "    #update beta and snp info\n",
    "    new_query = pd.concat([new_subject.iloc[:,:5],query[keep_idx].iloc[:,5:]],axis=1)\n",
    "    new_query.STAT[pm.sign_flip] = -new_query.STAT[pm.sign_flip]\n",
    "    return new_query,new_subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "965708e1-76bb-4348-8a00-f2fb75636539",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "def pair_match(a1,a2,ref1,ref2):\n",
    "    # a1 and a2 are the first data-set\n",
    "\t# ref1 and ref2 are the 2nd data-set\n",
    "\t# Make all the alleles into upper-case, as A,T,C,G:\n",
    "    a1 = a1.str.upper()\n",
    "    a2 = a2.str.upper()\n",
    "    ref1 = ref1.str.upper()\n",
    "    ref2 = ref2.str.upper()\n",
    "\t# Strand flip, to change the allele representation in the 2nd data-set\n",
    "    flip1 = ref1.apply(strand_flip)\n",
    "    flip2 = ref2.apply(strand_flip)\n",
    "    result = {}\n",
    "    result[\"ambiguous\"] = ((a1==\"A\") & (a2==\"T\")) | ((a1==\"T\") & (a2==\"A\")) | ((a1==\"C\") & (a2==\"G\")) | ((a1==\"G\") & (a2==\"C\"))\n",
    "    # as long as scenario 1 is involved, sign_flip will return TRUE\n",
    "    result[\"sign_flip\"] = ((a1==ref2) & (a2==ref1)) | ((a1==flip2) & (a2==flip1))\n",
    "\t# as long as scenario 2 is involved, strand_flip will return TRUE\n",
    "    result[\"strand_flip\"] = ((a1==flip1) & (a2==flip2)) | ((a1==flip2) & (a2==flip1))\n",
    "\t# remove other cases, eg, tri-allelic, one dataset is A C, the other is A G, for example.\n",
    "    result[\"exact_match\"] = ((a1 == ref1) & (a2 == ref2))\n",
    "    return pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "99554a33-7e23-468f-93bd-01ee3eb2e093",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "def strand_flip(s):\n",
    "    return ''.join(Seq(s).reverse_complement())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e63b670-435a-44ea-91a4-4a754dd2fe2a",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "kernels": [
    [
     "SoS",
     "sos",
     "",
     ""
    ]
   ],
   "version": "0.22.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
